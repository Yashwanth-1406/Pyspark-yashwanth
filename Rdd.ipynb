{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWLLukt8qoa7aG+x/INvNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashwanth-1406/Pyspark-yashwanth/blob/main/Rdd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRuAmwlxmji2",
        "outputId": "9653078e-d851-4215-ce97-ea0129a6d634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Even Number: [2]\n",
            "One Odd Number: [1]\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"EvenOddExample\")\n",
        "\n",
        "# Create an RDD with numbers from 1 to 10\n",
        "numbers_rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Filter even numbers\n",
        "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Filter odd numbers\n",
        "odd_rdd = numbers_rdd.filter(lambda x: x % 2 != 0)\n",
        "\n",
        "# Collect and print one even and one odd number\n",
        "print(\"One Even Number:\", even_rdd.take(1))  # Example: [2]\n",
        "print(\"One Odd Number:\", odd_rdd.take(1))    # Example: [1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"John\", \"HR\", 5000),\n",
        "    (2, \"Jane\", \"IT\", 8000),\n",
        "    (3, \"Mike\", \"IT\", 6000),\n",
        "    (4, \"Sara\", \"Finance\", 7000),\n",
        "    (5, \"David\", \"HR\", 5500)\n",
        "]\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Za37q1nmhX",
        "outputId": "c1585f13-199c-451c-f381-f196a7cf2134"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Name\",\"Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t2NXQz370B8",
        "outputId": "6523f3b1-94b0-4871-b61d-526b13f20334"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "| John|  5000|\n",
            "| Jane|  8000|\n",
            "| Mike|  6000|\n",
            "| Sara|  7000|\n",
            "|David|  5500|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VNWg7Qlr98-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Add new column: Salary_After_Tax = Salary * 0.8 (i.e., 20% tax deducted)\n",
        "df_with_tax = df.withColumn(\"Salary_After_Tax\", col(\"Salary\") * 0.8)\n",
        "\n",
        "# Now sort by Salary_After_Tax descending\n",
        "df_with_tax.sort(col(\"Salary_After_Tax\").desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tssGWL70EZ",
        "outputId": "1eb9974c-0483-477a-c9a4-1f01db1c3441"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------------+\n",
            "| ID| Name|Department|Salary|Salary_After_Tax|\n",
            "+---+-----+----------+------+----------------+\n",
            "|  2| Jane|        IT|  8000|          6400.0|\n",
            "|  4| Sara|   Finance|  7000|          5600.0|\n",
            "|  3| Mike|        IT|  6000|          4800.0|\n",
            "|  5|David|        HR|  5500|          4400.0|\n",
            "|  1| John|        HR|  5000|          4000.0|\n",
            "+---+-----+----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Get SparkContext from SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Create RDD of Row objects\n",
        "rdd = sc.parallelize([Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30)])\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "df = spark.createDataFrame(rdd)\n",
        "\n",
        "# Filter rows where age > 25\n",
        "filtered_df = df.filter(df.age > 25)\n",
        "\n",
        "# Select 'name' column from filtered DataFrame\n",
        "filtered_df.select(\"name\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8jfu_ga70G1",
        "outputId": "3a987946-2b43-482e-9184-59b7774e443c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Bob|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQLBasics\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"Sales\", 4500),\n",
        "    (5, \"Eva\", \"IT\", 4200)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFS96QtW70Jb",
        "outputId": "f97ad4b8-c77a-4795-d244-549715118126"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--_B8kCt70L-",
        "outputId": "f18ac55b-6337-485b-9fa7-c1f2cd2a2fdc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "# Generate 30 records with random data\n",
        "names = [\"John\", \"Jane\", \"Mike\", \"Sara\", \"David\", \"Emily\", \"George\", \"Nina\", \"Tom\", \"Anna\"]\n",
        "departments = [\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
        "salaries = [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
        "\n",
        "# Create and open a CSV file for writing\n",
        "with open('employee_data.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow([\"ID\", \"Name\", \"Department\", \"Salary\"])\n",
        "\n",
        "    # Write the 30 records\n",
        "    for i in range(1, 31):\n",
        "        name = random.choice(names)\n",
        "        department = random.choice(departments)\n",
        "        salary = random.choice(salaries)\n",
        "        writer.writerow([i, name, department, salary])\n",
        "\n",
        "print(\"CSV file 'employee_data.csv' has been generated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CT3oUWr70Od",
        "outputId": "e4760e2c-b2e4-435a-c782-7711be72a2ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'employee_data.csv' has been generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType,StructType,IntegerType\n",
        "\n",
        "schema = StructType() \\\n",
        "      .add(\"EmpID\", IntegerType()) \\\n",
        "      .add(\"Name\", StringType()) \\\n",
        "      .add(\"Department\",StringType())\\\n",
        "      .add(\"Salary\", IntegerType())\n",
        "print(schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUqxARi470RK",
        "outputId": "7d32e4ec-1c2a-4c7f-b03e-9930e371c353"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructType([StructField('EmpID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Department', StringType(), True), StructField('Salary', IntegerType(), True)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"sparkSQLBasics\").getOrCreate()\n",
        "stream_df=spark.readStream \\\n",
        "   .option(\"sep\",\",\") \\\n",
        "   .schema(schema) \\\n",
        "   .csv(\"/content/empdata/\")\n",
        "\n",
        "stream_df.isStreaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ2bG3PbvuPv",
        "outputId": "a48d2b32-bb9a-49c3-b718-f3abfe2762e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "transformed_df = stream_df.withColumn(\"NameUPPER\",upper(\"Name\"))"
      ],
      "metadata": {
        "id": "wGn2PcLawrR4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4WP9_Vj70Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = transformed_df.writeStream \\\n",
        "  .outputMode(\"append\") \\\n",
        "  .format(\"console\") \\\n",
        "  .start()"
      ],
      "metadata": {
        "id": "Ht4tgeT170WJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat<<EOF > /content/empdata.employee_data2.csv\n",
        "1,John,Sales,3000\n",
        "2,Jane,IT,4000\n",
        "3,Mike,Sales,5000\n",
        "4,Sara,Finance,6000\n",
        "5,David,HR,7000\n",
        "6,Emily,Marketing,6000\n",
        "7,George,HR,4000\n",
        "8,Nina,Sales,5000\n",
        "9,Tom,IT,8000\n",
        "10,Anna,Marketing,3000\n",
        "EOF"
      ],
      "metadata": {
        "id": "bWg_Xkjl70Ti"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = transformed_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"path\", \"/tmp/stream_json_output\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/stream_checkpoint\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "uJRp3K4E70bt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"/tmp/stream_json_output\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4WPLyOG70eV",
        "outputId": "9f0d113e-c0ef-48a3-8249-8052b4352ea8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+---------+------+\n",
            "|Department|EmpID|  Name|NameUPPER|Salary|\n",
            "+----------+-----+------+---------+------+\n",
            "|Department| NULL|  Name|     NAME|  NULL|\n",
            "| Marketing|    1|   Tom|      TOM| 10000|\n",
            "|     Sales|    2|George|   GEORGE| 10000|\n",
            "| Marketing|    3|  Jane|     JANE| 10000|\n",
            "|        HR|    4|  John|     JOHN|  4000|\n",
            "| Marketing|    5|  Nina|     NINA|  9000|\n",
            "|        HR|    6| Emily|    EMILY|  4000|\n",
            "| Marketing|    7| Emily|    EMILY|  3000|\n",
            "|     Sales|    8|  Jane|     JANE|  7000|\n",
            "| Marketing|    9|  Anna|     ANNA|  6000|\n",
            "|        HR|   10|George|   GEORGE|  4000|\n",
            "| Marketing|   11| Emily|    EMILY|  6000|\n",
            "|        IT|   12|  Anna|     ANNA|  8000|\n",
            "| Marketing|   13|  Sara|     SARA|  3000|\n",
            "|   Finance|   14|  Anna|     ANNA|  3000|\n",
            "| Marketing|   15|  Jane|     JANE|  3000|\n",
            "|     Sales|   16|George|   GEORGE|  3000|\n",
            "|        HR|   17|  Nina|     NINA|  4000|\n",
            "|        HR|   18|George|   GEORGE|  9000|\n",
            "|     Sales|   19|  John|     JOHN|  6000|\n",
            "+----------+-----+------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgg0Ickz70g_",
        "outputId": "f6d7c04f-ff80-4aab-bcc9-c69209167d76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.4.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.4.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "# Create Faker instance\n",
        "fake = Faker()\n",
        "\n",
        "# Output file\n",
        "output_file = \"/content/empdata/random_data.csv\"\n",
        "\n",
        "while True:\n",
        "    emp_id = random.randint(1000, 9999)      # Random Employee ID\n",
        "    name = fake.name()                       # Random Name\n",
        "    department = fake.job()                  # Random Job Title as Department\n",
        "    salary = random.randint(3000, 12000)     # Random Salary\n",
        "    unique_id = uuid.uuid4()                 # Unique ID for no repetition\n",
        "\n",
        "    record = f\"{emp_id},{name},{department},{salary},{unique_id}\\n\"\n",
        "\n",
        "    # Append to file\n",
        "    with open(output_file, \"a\") as f:\n",
        "        f.write(record)\n",
        "\n",
        "    print(\"Generated:\", record.strip())\n",
        "\n",
        "    time.sleep(1)  # Wait 1 second before generating next line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs5D-9hx70j1",
        "outputId": "cbc9f2a9-bb4e-4bf8-c488-7dd6bc4f1d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: 9056,Michael Richmond,Farm manager,7089,d02da143-bffd-4ce1-801d-84ee058d1386\n",
            "Generated: 3826,Dr. Phillip Davis,Public relations officer,7174,8b4a1101-d2fc-4676-9e90-faeb091f1214\n",
            "Generated: 3580,Raymond Smith,Social worker,9100,600b220f-52aa-4104-ad0f-9c593adf3d62\n",
            "Generated: 2569,Andrew Butler,Airline pilot,4565,aff7eca1-23c7-4cef-a0f3-ed7b7a1a6584\n",
            "Generated: 3485,David Ford,Medical secretary,7772,068a02a0-d26e-4ebd-b237-2c82698f690d\n",
            "Generated: 6537,Christopher Garrison,Chartered public finance accountant,4137,a7eb83b3-ecfd-4805-97d1-a7b4dc7ab00f\n",
            "Generated: 1554,Justin Russell,Engineer, electrical,8344,6daab10d-25a6-4f96-b374-bdc10c52a17a\n",
            "Generated: 9201,Daniel Stephenson,Cytogeneticist,7717,d140da8f-564e-4825-9812-1321cdba6ef2\n",
            "Generated: 7398,Nicole Boyd,Mudlogger,10004,7555a475-fcf9-4046-b1fe-bde259cfd246\n",
            "Generated: 3046,Debra Harrison,Meteorologist,5314,8e8be89a-b6dc-4b86-a2d6-5ff7928c75ab\n",
            "Generated: 5257,Alexander Gomez,Presenter, broadcasting,8756,660406b8-0399-4b08-907b-2febb74a2f23\n",
            "Generated: 5539,Jennifer Roberts,Chief Marketing Officer,8747,432fc5bd-2df3-423d-b78a-88558aa8a020\n",
            "Generated: 9427,Nicholas Whitney,Accounting technician,8731,cd1fae13-bbe8-41cc-811d-000690f5062e\n",
            "Generated: 3507,Brandy Taylor,Cytogeneticist,3096,edcd18a6-09a9-44aa-be28-68711fb2960c\n",
            "Generated: 9058,Dillon Peterson,Broadcast engineer,4172,b6b3cb9a-2984-4124-9cbd-6043d3411bef\n",
            "Generated: 2975,Mrs. Peggy Lee,Building control surveyor,11622,2bb4e78b-5553-465c-9c21-72313c1540e8\n",
            "Generated: 1465,Steven Harris,Therapist, nutritional,8215,04ec926d-a8ad-4fec-9b94-3de1031578b4\n",
            "Generated: 7297,Kevin Howell,Restaurant manager,4204,4f8f978f-0939-463f-8e49-59e4e196fd24\n",
            "Generated: 4977,Carlos Smith,Surveyor, mining,5254,58948c30-0755-4d99-8f1e-7966413d8cd9\n",
            "Generated: 7627,Edward Newman,Investment analyst,8688,ddddf4df-ed1d-4713-b143-c54c7abc5bff\n",
            "Generated: 5952,Kimberly Scott,Insurance claims handler,11429,03905e6b-8c1b-45a7-b5a6-2b28dc9f181b\n",
            "Generated: 1640,Matthew Navarro,Teacher, English as a foreign language,5295,578fb1d0-8b86-4079-a6b6-f19b5724cc25\n",
            "Generated: 4866,Brandon Gonzales,Glass blower/designer,3253,f6d48b89-d435-4119-868b-0565d76f220f\n",
            "Generated: 7824,Benjamin Willis,Financial trader,7375,9b18b044-855b-4a82-9453-e14e5be7643a\n",
            "Generated: 1813,Luke Collins,Retail banker,9097,784c8965-da14-44d3-bb90-da08bf7e7083\n",
            "Generated: 7435,Marie Gonzalez,Building control surveyor,4412,bbee9479-f56a-434a-8ea7-9f576db26dae\n",
            "Generated: 7505,Fernando Lee,Podiatrist,9155,d3455941-1b9e-43df-b65e-adaec15d0f4c\n",
            "Generated: 6408,Heather Long,Copywriter, advertising,4216,3440daad-0731-4b78-bf01-c26e24142023\n",
            "Generated: 7889,Lee Cooper,Farm manager,5104,a1b8b437-8983-4fe1-9a61-719aa918e455\n",
            "Generated: 1213,Tina Barnes,Psychotherapist,10818,4c9e6e7e-4505-41f6-9ebb-805a2eff8139\n",
            "Generated: 8039,Brenda Rodriguez,Warehouse manager,11613,6737ec5f-f018-4d3d-a462-6805ba2bcfe7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AebFjNxx70me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj4TmWBl70pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiFkpqYV70r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsAxhSIr70uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjevMLUn70w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-SbD58c70zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_g3jx3St702B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTtZWbbz704W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gyhv_Im97062"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbHB6H4_70-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60df6277"
      },
      "source": [
        "import os\n",
        "os.makedirs('/content/empdata', exist_ok=True)\n",
        "os.rename('/content/employee_data.csv', '/content/empdata/employee_data.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}